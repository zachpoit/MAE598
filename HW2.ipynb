{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAE598 HW 2\n",
    "\n",
    "Name: Zachary Poit\n",
    "\n",
    "### Question 1: \n",
    "\n",
    "Function: $ 2 x_1^2 - 4 x_1 x_2 + 1.5 x_2^2 + x_2$\n",
    "\n",
    "$ \\nabla f = \n",
    "\\begin{bmatrix}\n",
    "4 (x_1 - x_2) \\\\\n",
    "-4 x_1 + 3 x_2 + 1 \\\\\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "$ H_f = \n",
    "\\begin{bmatrix}\n",
    "4 & -4 \\\\\n",
    "-4 & 3 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$ \\left| H - I \\lambda \\right| = \\begin{bmatrix}\n",
    "4- \\lambda & -4 \\\\\n",
    "-4 & 3 -\\lambda\\\\\n",
    "\\end{bmatrix} = \\lambda^2 - 7 \\lambda -4$ \n",
    "\n",
    "$\\lambda_{1,2} = \\frac{7 \\pm \\sqrt{65}}{2}$ \n",
    "\n",
    "The eigen values are not all positive or all negitive or zero so the Hessian is a indefinite matrix. So we have a saddle!\n",
    "\n",
    "Taylor expansion at saddle point (Gradient is zero): $ f(x_1,x_2) = f(1,1) + \\frac{1}{2} \\begin{bmatrix}\n",
    "x_1 - 1 & x_2 - 1\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "4 & -4 \\\\\n",
    "-4 & 3 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "x_1 - 1 \\\\\n",
    " x_2 - 1\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$ f(x_1,x_2) = f(1,1) + \\frac{1}{2} \\begin{bmatrix}\n",
    "\\partial x_1 & \\partial x_2\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "4 & -4 \\\\\n",
    "-4 & 3 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\partial x_1 \\\\\n",
    "\\partial x_2\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$ f(x_1,x_2) = f(1,1) + \\frac{1}{2} \\left( 4 \\partial x_1^2 - 8 \\partial x_1 \\partial x_2 + 3 \\partial x_2^2\\right) $\n",
    "\n",
    "$ f(x_1,x_2) = f(1,1) + \n",
    "\n",
    "Factor this to put it in form $ f(x_1,x_2) = f(1,1) + \\left( a \\partial x_1 - b \\partial x_2 \\right)\\left( c \\partial x_1 - d \\partial x_2 \\right)$\n",
    "\n",
    "We get the following equations:\n",
    "\n",
    "$ ac = 2$\n",
    "\n",
    "$bd = 1.5$\n",
    "\n",
    "$ad + bc = 4$\n",
    "\n",
    "If we set $ a = 1 $ then, $ c = 2 $, $ d = 1 $, and $ b = \\frac{3}{2}$\n",
    "\n",
    "$ f(x_1,x_2) = f(1,1) + \\left( \\partial x_1 - \\frac{3}{2} \\partial x_2 \\right)\\left( 2\\partial x_1 - \\partial x_2 \\right)$\n",
    "\n",
    "$ \\left( \\partial x_1 - \\frac{3}{2} \\partial x_2 \\right)\\left( 2\\partial x_1 - \\partial x_2 \\right) < 0$ \n",
    "\n",
    "Two cases for the directions: \n",
    "\n",
    "Case 1: $ \\left( \\partial x_1 - \\frac{3}{2} \\partial x_2 \\right) > 0$ and $\\left( 2\\partial x_1 - \\partial x_2 \\right) < 0$ \n",
    "\n",
    "Case 2: $ \\left( \\partial x_1 - \\frac{3}{2} \\partial x_2 \\right) < 0$ and $\\left( 2\\partial x_1 - \\partial x_2 \\right) > 0$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: \n",
    "\n",
    "#### Part a:\n",
    "\n",
    "Step 1: solve the problem with just multivarite Calculus concepts:\n",
    "\n",
    "Reference point:\n",
    "\n",
    "$ P = (-1,0,1)^T $\n",
    "\n",
    "A point on the plane:\n",
    "\n",
    "$ Q = (1,0,0)^T $\n",
    "\n",
    "vector from Q to P:\n",
    "\n",
    "$ v = P - Q = (-2, 0, 1)^T $\n",
    "\n",
    "Normal vector: \n",
    "\n",
    "$ \\hat{n} = \\frac{(1,2,3)^T}{\\sqrt{1^2 + 2^2 + 3^2}} = \\left(\\frac{1}{\\sqrt{14}},\\frac{2}{\\sqrt{14}},\\frac{3}{\\sqrt{14}}\\right)^T$\n",
    "\n",
    "Find distance shortest distance between reference point and plane\n",
    "\n",
    "$D = v^T \\hat{n} = (-2, 0, 1) \\left(\\frac{1}{\\sqrt{14}},\\frac{2}{\\sqrt{14}},\\frac{3}{\\sqrt{14}}\\right)^T = \\frac{1}{\\sqrt{14}}$\n",
    "\n",
    "Subtract that distance times normal vector from reference point to get answer!\n",
    "\n",
    "$ A = P - D*\\hat{n} = (-1,0,1)^T - \\left(\\frac{1}{14},\\frac{2}{14},\\frac{3}{14}\\right)^T = \\left(-\\frac{15}{14},-\\frac{1}{7},\\frac{11}{14}\\right)^T$\n",
    "\n",
    "Step 2: Solve the problem using optimization:\n",
    "\n",
    "Distance from any point to the reference point is the 2 - norm. \n",
    "\n",
    "$ D = \\sqrt{(x_1 + 1)^2 + x_2^2 + (x_3 - 1)^2} $\n",
    "\n",
    "Minimizing $D^2$ will minimize $D$.\n",
    "\n",
    "So is $f(x) = (x_1 + 1)^2 + x_2^2 + (x_3 - 1)^2$ convex? \n",
    "\n",
    "The Hesssian of this matrix is:\n",
    "\n",
    "$ H_f = \n",
    "\\begin{bmatrix}\n",
    "10 & 12 \\\\\n",
    "12 & 20 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "The eigenvalues of the hessian matrix are 2 and 28. So the distance squared is a convex function. \n",
    "\n",
    "There is also $x_1 + 2x_2 + 3x_3 = 1$ which is a convex set (convex plane) which constrains $x_1, x_2, x_3$ to the plane.\n",
    "\n",
    "To make it unconstrained optimization problem, substitute into the convex function. \n",
    "\n",
    "$ f(x) = (1 - 2x_2 - 3x_3 + 1)^2 + x_2^2 + (x_3 - 1)^2 $\n",
    "\n",
    "#### Part B:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Gradient Descent (Inexact Line Search):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 64\n",
      "X1 = -1.0714285691906489\n",
      "X2 = -0.14285711580230312\n",
      "X3 = 0.7857142669317517\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# function to compute gradient at a given x value\n",
    "def grad(x):\n",
    "    dfdx_0 = 2*(5*x[0] + 6*x[1] - 4) \n",
    "    dfdx_1 = 2*(6*x[0] + 10*x[1] - 7)\n",
    "    return np.array([dfdx_0,dfdx_1])\n",
    "\n",
    "# function to evaluate D^2\n",
    "def distance(x):\n",
    "    return (2 - 2*x[0] - 3*x[1]) ** 2 + (x[0]) ** 2 + (x[1] - 1) ** 2\n",
    "\n",
    "# Recursive function that does the line search reduction of alpha\n",
    "def reduce_alpha_grad(x,alpha):\n",
    "    phi = distance(x) - t*np.dot(grad(x),grad(x))*alpha         # Calculate Phi\n",
    "    if (distance(x - alpha*grad(x)) > phi):                     # If alpha does not meet criteria, cut it in half\n",
    "        alpha = 0.5* alpha\n",
    "        return reduce_alpha_grad(x,alpha)                       # Have the function call itself (recursion)\n",
    "    else: \n",
    "        return alpha                                            # Otherwise return alpha\n",
    "\n",
    "x0 = np.array([1,0])                                           \n",
    "t = .5\n",
    "alpha = 1\n",
    "\n",
    "distance_list_grad = [x0]\n",
    "k = 0\n",
    "while (np.linalg.norm(grad(distance_list_grad[k])) > 0.0000001):\n",
    "    old = distance_list_grad[k]\n",
    "    alpha = 1\n",
    "    alpha = reduce_alpha_grad(old,alpha)\n",
    "    new = old - alpha*grad(old)\n",
    "    distance_list_grad.append(new)\n",
    "    k += 1\n",
    "    # print(new)\n",
    "\n",
    "print(\"Iterations: \" + str(len(distance_list_grad)-1))\n",
    "\n",
    "x2x3 = distance_list_grad[len(distance_list_grad)-1]\n",
    "\n",
    "x_2 = x2x3[0]\n",
    "x_3 = x2x3[1]\n",
    "x_1 = 1 - 2*x_2 - 3*x_3\n",
    "\n",
    "print(\"X1 = \" + str(x_1))\n",
    "print(\"X2 = \" + str(x_2))\n",
    "print(\"X3 = \" + str(x_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newtons Method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 1\n",
      "X1 = -1.0714285714285712\n",
      "X2 = -0.14285714285714346\n",
      "X3 = 0.785714285714286\n"
     ]
    }
   ],
   "source": [
    "def Hess(x): \n",
    "    dfd2x = np.zeros([2,2])\n",
    "    dfd2x[0,0] = 10\n",
    "    dfd2x[0,1] = 12\n",
    "    dfd2x[1,0] = dfd2x[0,1]\n",
    "    dfd2x[1,1] = 20\n",
    "    \n",
    "    return dfd2x\n",
    "\n",
    "distance_list_newt = [x0]\n",
    "k = 0\n",
    "\n",
    "while (np.linalg.norm(grad(distance_list_newt[k])) > 0.0000001):\n",
    "    old = distance_list_grad[k]\n",
    "    h = -np.linalg.solve(Hess(old),grad(old))\n",
    "    new = old + h\n",
    "    distance_list_newt.append(new)\n",
    "    k += 1\n",
    "\n",
    "print(\"Iterations: \" + str(len(distance_list_newt)-1))\n",
    "\n",
    "x2x3 = distance_list_newt[len(distance_list_newt)-1]\n",
    "\n",
    "x_2 = x2x3[0]\n",
    "x_3 = x2x3[1]\n",
    "x_1 = 1 - 2*x_2 - 3*x_3\n",
    "\n",
    "print(\"X1 = \" + str(x_1))\n",
    "print(\"X2 = \" + str(x_2))\n",
    "print(\"X3 = \" + str(x_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "\n",
    "Define function $f(x) = a^T x = a_1 x_1 + a_2 x_2 + ... + a_n x_n$\n",
    "\n",
    "$\\nabla f = a$\n",
    "\n",
    "$H_f = 0_{nxn}$\n",
    "\n",
    "Since the Hessian is Positive semi definite, the function is convex. (It is also negitive semi definite, so also concave)\n",
    "\n",
    "If $f(x)$ is convex then $ S_1 = \\{ x \\in \\mathbb{R}^n | f(x) \\leq c \\}$ is convex set. This is a half hyper space\n",
    "\n",
    "The other half hyper space $ S_2 = \\{ x \\in \\mathbb{R}^n | f(x) \\geq c \\}$ is also convex. \n",
    "\n",
    "The intersection of these two half hyper spaces gives us the set $ S_1 \\cap S_2 = S_3 = \\{ x \\in \\mathbb{R}^n | f(x) = c \\}$ which is the hyper plane \n",
    "\n",
    "The intersection of two convex sets are convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: \n",
    "\n",
    "For any $x \\in \\mathbb{R}$, the function $f(y) = xy - c(x)$ is a line. The line is convex. \n",
    "\n",
    "For every $x \\in \\mathbb{R}$, I make a line and put them all together in a set of functions. \n",
    "\n",
    "All of the lines have a unique slope based on their unique x. \n",
    "\n",
    "So I have a bunch of lines with the form $C_1 y + C_2$ in the set and I take point-wise maximum all the lines in the set. \n",
    "\n",
    "The maximum of any number of convex functions is also convex, so $C^{\\star}(y)$ is convex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.MAE598_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62cacf7a938ba19e01fdfa2ab3b94b36ae0a642c472d49d6b134c6285fea7ac7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
